<5장>

1. 완전 연결층은 전역패턴을 학습하고, 합성곱 층은 지역 패턴을 학습함
- 평행 이동 불변성
- 공간적 계층구조

2. 경계 문제와 패딩, 스트라이드로 인해 크기 달라질 수 있음

3. Max Pooling을 사용하여 크기를 절반으로 줄이는 이유
- 공간적 계층 구조 학습
- 너무 많은 파라미터

4. ImageDataGenerator를 이용하여 데이터 전처리(강아지, 고양이 사진)
- 무한히 반복하기 때문에 break가 반드시 필요
- yield 키워드 사용

5. 데이터 증식 : 더 많은 데이터를 생성하여 일반화에 기여(overfitting 방지)
- val, test는 증식되어선 안된다

6. 사전 훈련된 컨브넷 VGG16 사용 : 데이터가 부족할 때 일반적인 특성을 학습하는데 효과적

7. 특성 추출 : 사전 훈련된 Convolution Base는 동결하여 그대로 사용하고,
             완전 연결층 분류기 문제에 맞게 학습
             // 동결 후 반드시 compile
             
8. 완전 연결층의 학습 방법
- Convolutional base의 Output을 디스크에 저장한 후 Dense층의 Input으로 사용 (비용 적지만 데이터 증식 불가)
- conv_base 위에 dense 층을 쌓아 한 모델로 만듬 (증식 가능하지만 비용이 매우 비쌈)

9. 미세 조정 : 동결했던 상위 층 몇 개를 해제함으로써 문제에 맞는 특성을 더 학습함
- 하위 층은 저수준이기때문에 일반적인 특성 -> 망가지면 아쉬움
- 전체를 조정해버리면 모든 층이 문제에 맞는 특성을 학습하기 때문에 과대적합

10. 활성화 채널 추출
- 첫 번째 층은 에지 감지기,, 거의 모든 정보 유지
- 상위 층으로 갈수록 이미지 정보는 줄어들고 클래스 정보가 증가(고수준), 알아보기 힘듬
- 비어있는 활성화 많아짐

11. 경사 상승법을 사용하는 이유
- loss를 계산할 방법이 없어 스스로 정의
- 필터가 기존 이미지 방향으로 가기 때문에 weight의 진행 방향이 반대


<6장>
1. 토큰화 : 단어, 문자, n-그램으로 쪼개는 것 -> 토큰을 벡터에 연결하는 것이 벡터화

2. 원-핫 인코딩
- zip( )은 각 리스트에서 같은 열에 속한 원소들끼리 묶는 것

3. 원-핫 해싱
- 단어에 명시적인 인덱스를 부여하고, 고정된 크기의 벡터로 변환하는 것
- 해싱 충돌 조심

4. 단어 임베딩
- 원-핫 인코딩은 sparse한데, 임베딩은 dense함
- 공간 절약할 수 있고, 단어를 학습할 수 있음
- 언어를 기하학적인 공간에 매핑하는 것

5. Embedding 층으로 단어 임베딩

6. 사전 훈련된 단어 임베딩
- 일반적인 특성 사용(GloVe)

7. 순환 신경망(Recurrent Neural Network) : 이전의 output을 state에 저장한 뒤, 이후에 반영하여 학습하는 신경망
타임스탭 : RNN을 몇 개로 unfold할 것인지에 대한 수치

8. RNN은 타임스탭을 모은 전체 출력 시퀀스를 반환(중간층)
or 이론적으로 모든 타임스탭의 정보가 담긴 마지막 output 반환(마지막 층)

9. LSTM, GRU : SimpleRNN의 그래디언트 베니싱 문제 해결
이동 상태 개념을 도입하여, 저장한 state에 적당한 알고리즘을 적용하고, 여러 타임스탭에 걸쳐 전달

10. 온도 측정 모델
- 순환 드롭아웃 : 한 타임스탭에서는 같은 드롭아웃 마스크를 사용하는 것
- 스태킹 순환 층 : 순환 층을 쌓아올려 네트워크의 크기를 증가시키는 것
- 양방향 순환 층 : 반대 방향으로 학습하여 정방향에서 놓쳤던 패턴들을 학습하는 것(앙상블)
  Bidirectional(layers.LSTM(32))

11. axis=0은 행렬에서 열 단위로 연산을 하겠다는 뜻











